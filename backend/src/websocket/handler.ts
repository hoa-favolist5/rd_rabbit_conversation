import { WebSocket } from "ws";
import { v4 as uuidv4 } from "uuid";
import { chat, needsMovieSearch } from "../services/claude.js";
import { synthesizeSpeechBase64 } from "../services/azure-tts.js";
import { searchMovies } from "../db/movies.js";
import { startTimer } from "../utils/timer.js";
import type {
  ConversationTurn,
  ConversationStatus,
  EmotionType,
  StatusMessage,
  UserMessage,
  AssistantMessage,
  ErrorMessage,
  WSMessage,
} from "../types/index.js";

// Configuration for parallel TTS streaming (TEN Framework inspired)
const ENABLE_PARALLEL_TTS = true;
const MIN_SENTENCE_LENGTH_FOR_TTS = 5;
const MAX_CONCURRENT_TTS = 3;  // Limit concurrent TTS to avoid rate limiting
const SHORT_RESPONSE_THRESHOLD = 30;  // Skip chunking for very short responses

// Waiting phrases - played before database search (pre-recorded audio)
const WAITING_PHRASES = [
  "ã‚‚ã¡ã‚ã‚“ã€‚æ€¥ã„ã§ç¢ºèªã™ã‚‹ã‹ã‚‰ã€å¾…ã£ã¦ã¦ã­ã€‚",
  "äº†è§£ã€‚ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã­ã€‚",
  "ã†ã‚“ã€ã‚ã‹ã£ãŸã€‚ä»Šç¢ºèªã™ã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ã­ã€‚",
  "ãªã‚‹ã»ã©ã€‚å°‘ã—èª¿ã¹ã¦ã¿ã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ã€‚",
  "OKã€‚å†…å®¹ã‚’ç¢ºèªã™ã‚‹ã‹ã‚‰ã€å°‘ã€…ãŠå¾…ã¡ã‚’ã€‚",
  "ä»»ã›ã¦ã€‚ä¸å¯§ã«ãŠèª¿ã¹ã™ã‚‹ã‹ã‚‰ã€å°‘ã—å¾…ã£ã¦ã¦ã­ã€‚",
  "æ‰¿çŸ¥ã—ãŸã‚ˆã€‚ã¡ã‚‡ã£ã¨è€ƒãˆã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ãã‚Œã‚‹ï¼Ÿ",
  "ã‚ã€ãã®ã“ã¨ã ã­ã€‚ä»Šèª¿ã¹ã¦ã‚ã’ã‚‹ã‹ã‚‰å¾…ã£ã¦ã€‚",
  "ç¢ºã‹ã«ã€‚ã™ãç¢ºèªã™ã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ã€‚",
  "ã„ã„ã‚ˆã€‚èª¿ã¹ã¦ã¿ã‚‹ã‹ã‚‰ã€ãã“ã§å¾…ã£ã¦ã¦ã­ã€‚",
  "ã†ã‚“ã€ã‚ã‹ã‚‹ã‚ˆã€‚ã™ãã«èª¿ã¹ã¦ã¿ã‚‹ã­ã€‚",
  "ãŠã£ã‘ãƒ¼ã€‚æƒ…å ±ã‚’æ¢ã—ã¦ãã‚‹ã‹ã‚‰ã€å¾…ã£ã¦ã¦ã­ã€‚",
  "äº†è§£äº†è§£ã€‚è½ã¡ç€ã„ã¦èª¿ã¹ã‚‹ã‹ã‚‰ã€å¾…ã£ã¦ã¦ã­ã€‚",
  "ãã†ã ã­ã€‚ã™ãç¢ºèªã™ã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã€‚",
  "ãŠå®‰ã„å¾¡ç”¨ã ã‚ˆã€‚ä»Šã™ãèª¿ã¹ã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ã­ã€‚",
  "ã‚ªãƒƒã‚±ãƒ¼ã€‚ã—ã£ã‹ã‚Šæ¢ã—ã¦ã¿ã‚‹ã‹ã‚‰ã€å¾…ã£ã¦ã¦ã€‚",
  "ãã†ã ã‚ˆã­ã€‚ä»Šã€è©³ã—ãç¢ºèªã™ã‚‹ã‹ã‚‰ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã€‚",
  "ãŠã£ã‘ãƒ¼ã€‚ã™ãæº–å‚™ã™ã‚‹ã‹ã‚‰ã€ã¡ã‚‡ã£ã¨å¾…ã£ã¦ã¦ã­ã€‚",
  "æ•™ãˆã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚ä»Šã™ãèª¿ã¹ã‚‹ã‹ã‚‰å¾…ã£ã¦ã¦ã€‚",
  "äº†è§£ã—ãŸã‚ˆã€‚ã™ãã«è¦‹ã¤ã‘ã¦ãã‚‹ã‹ã‚‰ã€å¾…ã£ã¦ã¦ã­ã€‚",
];

// Workflow step definitions matching WORKFLOW.md
type WorkflowStep = 
  | "STEP1_TEXT_INPUT"
  | "STEP2_WEBSOCKET_SEND"
  | "STEP3_BACKEND_START"
  | "STEP4_LLM_REQUEST"
  | "STEP5_DB_SEARCH"
  | "STEP6_LLM_RESPONSE"
  | "STEP7_TEXT_RESPONSE"
  | "STEP8_TTS_SYNTHESIS"
  | "STEP9_AUDIO_SEND"
  | "STEP10_AUDIO_PLAY"
  | "STEP11_TIMING_SEND"
  | "STEP12_COMPLETE";

interface StepTiming {
  step: WorkflowStep;
  name: string;
  nameJa: string;
  durationMs: number;
  startTime: number;
  endTime: number;
  details?: Record<string, unknown>;
}

interface Session {
  id: string;
  ws: WebSocket;
  history: ConversationTurn[];
  status: ConversationStatus;
  pendingRequest: boolean;  // Prevent duplicate requests
}

// Active sessions
const sessions = new Map<string, Session>();

// TTS concurrency limiter (TEN Framework pattern)
let activeTTSCount = 0;
const ttsWaitQueue: Array<() => void> = [];

async function withTTSLimit<T>(fn: () => Promise<T>): Promise<T> {
  // Wait if at concurrency limit
  if (activeTTSCount >= MAX_CONCURRENT_TTS) {
    await new Promise<void>(resolve => ttsWaitQueue.push(resolve));
  }
  
  activeTTSCount++;
  try {
    return await fn();
  } finally {
    activeTTSCount--;
    // Wake up next waiting request
    const next = ttsWaitQueue.shift();
    if (next) next();
  }
}

// Step labels
const STEP_LABELS: Record<WorkflowStep, { name: string; nameJa: string }> = {
  STEP1_TEXT_INPUT: { name: "Text Input", nameJa: "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›" },
  STEP2_WEBSOCKET_SEND: { name: "WebSocket Send", nameJa: "WebSocketé€ä¿¡" },
  STEP3_BACKEND_START: { name: "Backend Start", nameJa: "Backendå‡¦ç†é–‹å§‹" },
  STEP4_LLM_REQUEST: { name: "LLM Request", nameJa: "LLM å‘¼ã³å‡ºã—" },
  STEP5_DB_SEARCH: { name: "DB Search", nameJa: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢" },
  STEP6_LLM_RESPONSE: { name: "LLM Response", nameJa: "Claudeæœ€çµ‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹" },
  STEP7_TEXT_RESPONSE: { name: "Text Response", nameJa: "ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”é€ä¿¡" },
  STEP8_TTS_SYNTHESIS: { name: "TTS Synthesis", nameJa: "Azure TTSéŸ³å£°åˆæˆ" },
  STEP9_AUDIO_SEND: { name: "Audio Send", nameJa: "éŸ³å£°ãƒ‡ãƒ¼ã‚¿é€ä¿¡" },
  STEP10_AUDIO_PLAY: { name: "Audio Play", nameJa: "éŸ³å£°å†ç”Ÿ" },
  STEP11_TIMING_SEND: { name: "Timing Send", nameJa: "ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±é€ä¿¡" },
  STEP12_COMPLETE: { name: "Complete", nameJa: "å®Œäº†" },
};

/**
 * Workflow Timer - tracks all steps in the conversation workflow
 */
class WorkflowTimer {
  private sessionId: string;
  private steps: StepTiming[] = [];
  private overallStart: number;
  private currentStepStart: number | null = null;
  private currentStep: WorkflowStep | null = null;
  private dbSearchTime: number = 0;
  private hasDbSearch: boolean = false;

  constructor(sessionId: string) {
    this.sessionId = sessionId;
    this.overallStart = performance.now();
  }

  /**
   * Start tracking a step
   */
  startStep(step: WorkflowStep, details?: Record<string, unknown>): void {
    this.currentStep = step;
    this.currentStepStart = performance.now();
  }

  /**
   * End current step
   */
  endStep(details?: Record<string, unknown>): void {
    if (this.currentStep && this.currentStepStart !== null) {
      const endTime = performance.now();
      const durationMs = Math.round(endTime - this.currentStepStart);
      const labels = STEP_LABELS[this.currentStep];

      this.steps.push({
        step: this.currentStep,
        name: labels.name,
        nameJa: labels.nameJa,
        durationMs,
        startTime: this.currentStepStart,
        endTime,
        details,
      });

      // Log step
      const color = durationMs < 100 ? "\x1b[32m" : durationMs < 500 ? "\x1b[33m" : "\x1b[31m";
      const reset = "\x1b[0m";
      console.log(
        `${color}â±ï¸  [${this.sessionId.slice(0, 8)}] ${this.currentStep}: ${durationMs}ms${reset}`
      );

      this.currentStepStart = null;
      this.currentStep = null;
    }
  }

  /**
   * Track a step with async function
   */
  async trackStep<T>(
    step: WorkflowStep,
    fn: () => Promise<T>,
    details?: Record<string, unknown>
  ): Promise<T> {
    this.startStep(step, details);
    try {
      const result = await fn();
      this.endStep(details);
      return result;
    } catch (error) {
      this.endStep({ ...details, error: true });
      throw error;
    }
  }

  /**
   * Record DB search time (tracked separately within LLM step)
   */
  recordDbSearch(durationMs: number): void {
    this.dbSearchTime = durationMs;
    this.hasDbSearch = true;
  }

  /**
   * Get summary with all steps
   */
  getSummary(): {
    sessionId: string;
    totalDurationMs: number;
    steps: StepTiming[];
    hasDbSearch: boolean;
    dbSearchTime: number;
  } {
    const totalDurationMs = Math.round(performance.now() - this.overallStart);
    return {
      sessionId: this.sessionId,
      totalDurationMs,
      steps: this.steps,
      hasDbSearch: this.hasDbSearch,
      dbSearchTime: this.dbSearchTime,
    };
  }

  /**
   * Log summary
   */
  logSummary(): void {
    const summary = this.getSummary();
    console.log("\nğŸ“Š Workflow Summary:");
    console.log(`   Session: ${this.sessionId.slice(0, 8)}`);
    console.log(`   Total: ${summary.totalDurationMs}ms`);
    console.log("   Steps:");
    for (const step of this.steps) {
      const percentage = ((step.durationMs / summary.totalDurationMs) * 100).toFixed(1);
      console.log(`     - ${step.nameJa}: ${step.durationMs}ms (${percentage}%)`);
    }
    if (this.hasDbSearch) {
      console.log(`     - (DB Search included: ${this.dbSearchTime}ms)`);
    }
    console.log("");
  }
}

/**
 * Send a message to the WebSocket client
 */
function send(ws: WebSocket, message: WSMessage): void {
  if (ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify(message));
  }
}

/**
 * Send status update to client
 */
function sendStatus(
  ws: WebSocket,
  status: ConversationStatus,
  emotion: EmotionType,
  statusText: string
): void {
  const message: StatusMessage = {
    type: "status",
    status,
    emotion,
    statusText,
  };
  send(ws, message);
}

/**
 * Send user message echo to client
 */
function sendUserMessage(ws: WebSocket, text: string): void {
  const message: UserMessage = {
    type: "user_message",
    text,
  };
  send(ws, message);
}

/**
 * Send assistant message to client
 */
function sendAssistantMessage(
  ws: WebSocket,
  text: string,
  emotion: EmotionType,
  messageId?: string
): void {
  const message: AssistantMessage = {
    type: "assistant_message",
    text,
    emotion,
    ...(messageId ? { messageId } : {}),
  };
  send(ws, message);
}

/**
 * Send assistant streaming delta
 */
function sendAssistantDelta(ws: WebSocket, text: string, messageId: string): void {
  send(ws, {
    type: "assistant_delta",
    text,
    messageId,
  });
}

/**
 * Send error message to client
 */
function sendError(ws: WebSocket, errorMessage: string): void {
  const message: ErrorMessage = {
    type: "error",
    message: errorMessage,
  };
  send(ws, message);
}

/**
 * Send waiting signal to client (before DB search)
 * Frontend plays pre-recorded audio from public/waiting/{index}.mp3
 */
function sendWaiting(ws: WebSocket): void {
  const index = Math.floor(Math.random() * WAITING_PHRASES.length);
  console.log(`â³ Sending waiting signal #${index}: "${WAITING_PHRASES[index]}"`);
  send(ws, {
    type: "waiting",
    index,
    text: WAITING_PHRASES[index],  // For debugging/fallback
  });
}

/**
 * Send workflow timing information to client
 */
function sendWorkflowTiming(
  ws: WebSocket,
  workflowTimer: WorkflowTimer,
  usedTool: boolean = false
): void {
  const summary = workflowTimer.getSummary();
  
  send(ws, {
    type: "workflow_timing",
    steps: summary.steps.map((s) => ({
      step: s.step,
      name: s.name,
      nameJa: s.nameJa,
      durationMs: s.durationMs,
    })),
    hasDbSearch: summary.hasDbSearch,
    dbSearchTime: summary.dbSearchTime,
    usedTool,
    totalMs: summary.totalDurationMs,
  });
}

/**
 * Process user text input and generate response
 */
async function processUserInput(session: Session, userText: string): Promise<void> {
  const { ws, history } = session;
  
  // Request deduplication: prevent duplicate requests while one is in flight
  if (session.pendingRequest) {
    console.log(`âš ï¸ [${session.id.slice(0, 8)}] Ignoring duplicate request (previous still processing)`);
    return;
  }
  session.pendingRequest = true;
  
  const workflow = new WorkflowTimer(session.id);

  try {
    // STEP 1 & 2: Already happened on client side, we mark as received
    workflow.startStep("STEP2_WEBSOCKET_SEND");
    workflow.endStep({ received: true });

    // STEP 3: Backend processing start
    workflow.startStep("STEP3_BACKEND_START");
    sendUserMessage(ws, userText);
    session.status = "thinking";
    sendStatus(ws, "thinking", "thinking", "è€ƒãˆä¸­...");
    workflow.endStep();

    // STEP 4-6: LLM Request and Response
    workflow.startStep("STEP4_LLM_REQUEST");

    const assistantMessageId = `assistant-${session.id}-${Date.now()}`;

    // Prefetch movie search in parallel when likely needed
    const shouldPrefetchMovies = needsMovieSearch(userText);
    const prefetchPromise = shouldPrefetchMovies
      ? (async () => {
          const prefetchTimer = startTimer("DB Search (Prefetch)", { query: userText });
          const result = await searchMovies(userText);
          const timing = prefetchTimer.stop();
          workflow.recordDbSearch(timing.durationMs);
          return result;
        })().catch(() => null)
      : null;

    // Parallel TTS: Queue sentences and synthesize while LLM is still streaming
    interface TTSChunkResult {
      audio: string;
      sentence: string;
      index: number;
      durationMs: number;
      charCount: number;
    }
    const ttsQueue: Promise<TTSChunkResult | null>[] = [];
    let sentenceIndex = 0;

    const response = await chat(
      history,
      userText,
      async (query, genre, year) => {
        // Track DB search within LLM step
        if (
          prefetchPromise &&
          query.trim() === userText.trim() &&
          !genre &&
          !year
        ) {
          const prefetched = await prefetchPromise;
          if (prefetched) {
            return prefetched;
          }
        }

        const dbTimer = startTimer("DB Search", { query, genre, year });
        const result = await searchMovies(query, genre, year);
        const dbTiming = dbTimer.stop();
        workflow.recordDbSearch(dbTiming.durationMs);
        return result;
      },
      (delta) => {
        // Stream partial text to client for faster perceived response
        sendAssistantDelta(ws, delta, assistantMessageId);
      },
      ENABLE_PARALLEL_TTS ? (sentence, emotion) => {
        // Parallel TTS: Start synthesizing each sentence immediately
        if (sentence.length >= MIN_SENTENCE_LENGTH_FOR_TTS) {
          const idx = sentenceIndex++;
          const startTime = performance.now();
          const charCount = sentence.length;
          
          console.log(`ğŸ™ï¸ [${session.id.slice(0, 8)}] TTS Chunk #${idx} START: "${sentence}" (${charCount} chars)`);
          
          // Use TTS concurrency limiter to avoid rate limiting
          const ttsPromise = withTTSLimit(() => 
            synthesizeSpeechBase64(sentence, {
              emotion,
              voice: "female",
            })
          ).then(audio => {
            const durationMs = Math.round(performance.now() - startTime);
            const audioSizeKB = Math.round(audio.length * 0.75 / 1024); // base64 to KB
            console.log(`âœ… [${session.id.slice(0, 8)}] TTS Chunk #${idx} DONE: ${durationMs}ms | ${charCount} chars | ${audioSizeKB}KB audio`);
            return { audio, sentence, index: idx, durationMs, charCount };
          }).catch(err => {
            const durationMs = Math.round(performance.now() - startTime);
            console.error(`âŒ [${session.id.slice(0, 8)}] TTS Chunk #${idx} FAILED after ${durationMs}ms:`, err);
            return null;
          });
          
          ttsQueue.push(ttsPromise);
        }
      } : undefined,
      // onToolUse callback - send waiting signal before DB search
      () => {
        sendWaiting(ws);
      }
    );

    workflow.endStep({
      inputLength: userText.length,
      usedTool: response.usedTool,
      hasToolUse: workflow.getSummary().hasDbSearch
    });

    if (response.usedTool) {
      console.log("ğŸ”§ Tool used: movie search (2 API calls)");
    } else {
      console.log("âš¡ No tool used: single API call");
    }

    // Update conversation history
    history.push({ role: "user", content: userText });
    history.push({ role: "assistant", content: response.text });

    // Limit history to last 20 turns
    if (history.length > 20) {
      history.splice(0, history.length - 20);
    }

    // STEP 7: Send text response
    workflow.startStep("STEP7_TEXT_RESPONSE");
    sendAssistantMessage(ws, response.text, response.emotion, assistantMessageId);
    session.status = "speaking";
    sendStatus(ws, "speaking", response.emotion, "è©±ã—ã¦ã„ã¾ã™...");
    workflow.endStep({ textLength: response.text.length, emotion: response.emotion });

    // STEP 8: TTS synthesis (parallel mode or fallback)
    workflow.startStep("STEP8_TTS_SYNTHESIS");
    
    let audioSent = false;
    
    // Short response optimization: skip chunking for very brief responses
    const useParallelTTS = ENABLE_PARALLEL_TTS && 
                           ttsQueue.length > 0 && 
                           response.text.length >= SHORT_RESPONSE_THRESHOLD;
    
    if (useParallelTTS) {
      // Process TTS chunks - send immediately as each completes (true parallel)
      console.log(`\nğŸ”Š [${session.id.slice(0, 8)}] Sending ${ttsQueue.length} TTS chunks as they complete...`);
      
      const totalChunks = ttsQueue.length;
      const chunkResults: TTSChunkResult[] = [];
      let sentCount = 0;
      
      // Process all chunks in parallel and send each as it completes
      const sendPromises = ttsQueue.map(async (promise, i) => {
        const result = await promise;
        if (result) {
          // Send immediately when this chunk is ready (don't wait for earlier chunks)
          send(ws, {
            type: "audio_chunk",
            data: result.audio,
            format: "mp3",
            index: result.index,
            total: totalChunks,
            isLast: result.index === totalChunks - 1,
          });
          sentCount++;
          chunkResults.push(result);
          return result;
        }
        return null;
      });
      
      // Wait for all to complete (but they've already been sent as they finished)
      await Promise.all(sendPromises);
      audioSent = sentCount > 0;
      
      // Calculate totals for logging
      const totalTTSTime = chunkResults.reduce((sum, c) => sum + c.durationMs, 0);
      const totalChars = chunkResults.reduce((sum, c) => sum + c.charCount, 0);
      const totalAudioKB = chunkResults.reduce((sum, c) => sum + Math.round(c.audio.length * 0.75 / 1024), 0);
      
      // Log detailed TTS summary (sorted by index)
      chunkResults.sort((a, b) => a.index - b.index);
      console.log(`\nğŸ“Š TTS Chunk Summary (${session.id.slice(0, 8)}):`);
      console.log(`${"â”€".repeat(80)}`);
      console.log(`| ${"#".padEnd(3)} | ${"Time".padEnd(8)} | ${"Chars".padEnd(6)} | ${"Content".padEnd(50)} |`);
      console.log(`|${"-".repeat(5)}|${"-".repeat(10)}|${"-".repeat(8)}|${"-".repeat(52)}|`);
      
      for (const chunk of chunkResults) {
        const content = chunk.sentence.length > 48 
          ? chunk.sentence.slice(0, 45) + "..." 
          : chunk.sentence;
        console.log(`| ${String(chunk.index).padEnd(3)} | ${(chunk.durationMs + "ms").padEnd(8)} | ${String(chunk.charCount).padEnd(6)} | ${content.padEnd(50)} |`);
      }
      
      console.log(`${"â”€".repeat(80)}`);
      console.log(`| ${"SUM".padEnd(3)} | ${(totalTTSTime + "ms").padEnd(8)} | ${String(totalChars).padEnd(6)} | Total audio: ${totalAudioKB}KB across ${chunkResults.length} chunks`.padEnd(53) + ` |`);
      console.log(`${"â”€".repeat(80)}\n`);
      
      workflow.endStep({ 
        mode: "parallel", 
        chunks: totalChunks,
        totalTTSTime,
        textLength: response.text.length 
      });
    }
    
    // Fallback: synthesize full response if parallel TTS didn't produce audio
    if (!audioSent) {
      console.log(`\nğŸ”Š [${session.id.slice(0, 8)}] Sequential TTS (fallback mode)...`);
      const seqStartTime = performance.now();
      
      try {
        const audioBase64 = await synthesizeSpeechBase64(response.text, {
          emotion: response.emotion,
          voice: "female",
        });
        
        const seqDuration = Math.round(performance.now() - seqStartTime);
        const audioKB = Math.round(audioBase64.length * 0.75 / 1024);
        
        console.log(`${"â”€".repeat(80)}`);
        console.log(`ğŸ“Š TTS Sequential Summary (${session.id.slice(0, 8)}):`);
        console.log(`   Content: "${response.text.slice(0, 60)}${response.text.length > 60 ? "..." : ""}"`);
        console.log(`   Chars: ${response.text.length} | Time: ${seqDuration}ms | Audio: ${audioKB}KB`);
        console.log(`${"â”€".repeat(80)}\n`);
        
        workflow.endStep({ 
          mode: "sequential",
          textLength: response.text.length, 
          durationMs: seqDuration,
          emotion: response.emotion 
        });

        // STEP 9: Send audio data
        workflow.startStep("STEP9_AUDIO_SEND");
        send(ws, {
          type: "audio",
          data: audioBase64,
          format: "mp3",
        });
        workflow.endStep({ audioSize: audioBase64.length });
        audioSent = true;
      } catch (ttsError) {
        console.error("TTS error:", ttsError);
        workflow.endStep({ error: true });
        send(ws, {
          type: "tts_error",
          message: "éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ",
        });
      }
    } else {
      // Mark audio send complete for parallel mode
      workflow.startStep("STEP9_AUDIO_SEND");
      workflow.endStep({ mode: "parallel_streaming" });
    }

    // STEP 11: Send timing info
    workflow.startStep("STEP11_TIMING_SEND");
    workflow.logSummary();
    sendWorkflowTiming(ws, workflow, response.usedTool);
    workflow.endStep();

    // STEP 12: Complete
    workflow.startStep("STEP12_COMPLETE");
    session.status = "idle";
    sendStatus(ws, "idle", response.emotion, "");
    workflow.endStep();

  } catch (error) {
    console.error("Process input error:", error);
    workflow.logSummary();
    session.status = "idle";
    sendStatus(ws, "idle", "confused", "");
    sendError(ws, "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚");
  } finally {
    // Always reset pending flag
    session.pendingRequest = false;
  }
}

/**
 * Handle incoming WebSocket message
 */
async function handleMessage(session: Session, data: string): Promise<void> {
  try {
    const message = JSON.parse(data) as WSMessage;

    switch (message.type) {
      case "text_input": {
        const text = message.text as string;
        if (text && text.trim()) {
          await processUserInput(session, text.trim());
        }
        break;
      }

      case "start_listening": {
        session.status = "listening";
        sendStatus(session.ws, "listening", "listening", "èã„ã¦ã„ã¾ã™...");
        break;
      }

      case "stop_listening": {
        session.status = "idle";
        sendStatus(session.ws, "idle", "neutral", "");
        break;
      }

      case "ping": {
        send(session.ws, { type: "pong" });
        break;
      }

      default:
        console.log("Unknown message type:", message.type);
    }
  } catch (error) {
    console.error("Handle message error:", error);
    sendError(session.ws, "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ");
  }
}

/**
 * Handle new WebSocket connection
 */
export function handleConnection(ws: WebSocket): void {
  const sessionId = uuidv4();
  
  const session: Session = {
    id: sessionId,
    ws,
    history: [],
    status: "idle",
    pendingRequest: false,
  };

  sessions.set(sessionId, session);
  console.log(`ğŸ“± Client connected: ${sessionId}`);

  // Send welcome message
  send(ws, {
    type: "connected",
    sessionId,
    message: "ãƒ©ãƒ“ãƒƒãƒˆAIã«æ¥ç¶šã—ã¾ã—ãŸï¼",
  });

  // Send initial status
  sendStatus(ws, "idle", "happy", "");

  // Send greeting
  setTimeout(async () => {
    const greetingTimer = startTimer("Greeting TTS");
    try {
      const greeting = "ã“ã‚“ã«ã¡ã¯ï¼";
      
      sendAssistantMessage(ws, greeting, "happy");
      session.history.push({ role: "assistant", content: greeting });

      // Generate greeting audio
      try {
        const audioBase64 = await synthesizeSpeechBase64(greeting, {
          emotion: "happy",
          voice: "female",
        });
        greetingTimer.stopAndLog(sessionId);
        
        send(ws, {
          type: "audio",
          data: audioBase64,
          format: "mp3",
        });
      } catch (ttsError) {
        console.error("Greeting TTS error:", ttsError);
        greetingTimer.stopAndLog(sessionId);
      }
    } catch (error) {
      console.error("Greeting error:", error);
      greetingTimer.stopAndLog(sessionId);
    }
  }, 500);

  // Handle incoming messages
  ws.on("message", (data) => {
    handleMessage(session, data.toString());
  });

  // Handle close
  ws.on("close", () => {
    sessions.delete(sessionId);
    console.log(`ğŸ“´ Client disconnected: ${sessionId}`);
  });

  // Handle errors
  ws.on("error", (error) => {
    console.error(`WebSocket error for ${sessionId}:`, error);
    sessions.delete(sessionId);
  });
}

/**
 * Get active session count
 */
export function getSessionCount(): number {
  return sessions.size;
}
