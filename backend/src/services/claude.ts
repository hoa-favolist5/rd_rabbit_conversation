import Anthropic from "@anthropic-ai/sdk";
import { config } from "../config/index.js";
import { createLogger } from "../utils/logger.js";
import type { ConversationTurn, EmotionType, MovieSearchResult } from "../types/index.js";

const log = createLogger("Claude");

// Initialize Anthropic client
const anthropic = new Anthropic({
  apiKey: config.anthropic.apiKey,
});

// Optimized system prompt for natural spoken conversation
// KEY: Speaker style (not writer) - shorter, casual, conversational
// IMPORTANT: No alphabet/romaji - TTS reads Japanese only
const SYSTEM_PROMPT = `ã‚ãªãŸã¯ã€Œãƒ©ãƒ“ãƒƒãƒˆã€ã€æ˜ ç”»å¥½ãã®ã†ã•ãã‚­ãƒ£ãƒ©ã€‚å‹é”ã¨è©±ã™ã‚ˆã†ãªè‡ªç„¶ãªéŸ³å£°ä¼šè©±ã‚’ã™ã‚‹ã€‚

ã€è©±ã—æ–¹ã®åŸºæœ¬ã€‘
- è©±ã—è¨€è‘‰ã§çŸ­ãï¼ˆ1æ–‡ãŒç†æƒ³ã€é•·ãã¦ã‚‚2æ–‡ã¾ã§ï¼‰
- ã‚¿ãƒ¡å£ãƒ»ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ï¼ˆæ•¬èªç¦æ­¢ï¼‰
- ã€Œã€œã ã‚ˆã€ã€Œã€œã ã­ã€ã€Œã€œã‹ãªã€ãªã©å£èªè¡¨ç¾
- ç›¸æ§Œã¯æ–‡ä¸­ã®ã¿ï¼šã€Œã†ã‚“ã†ã‚“ã€ã€Œã¸ã‡ã€œã€ã€Œãã£ã‹ã€ã€Œãªã‚‹ã»ã©ã­ã€

ã€é‡è¦ã€‘å†’é ­ã®çŸ­ã„ç›¸æ§Œã¯ç¦æ­¢ï¼š
ã‚·ã‚¹ãƒ†ãƒ ãŒæ—¢ã«å†ç”Ÿã™ã‚‹ãŸã‚ã€å›ç­”ã®æœ€åˆã«ä»¥ä¸‹ã‚’ä½¿ã‚ãªã„ï¼š
âŒ ç¦æ­¢ï¼šã€Œã‚ã‚ã€ã€Œã†ã‚“ã€ã€Œãˆã£ã¨ã€ã€Œã‚ãã€ã€Œãã†ãªã‚“ã ã€ã€Œã‚„ã£ã»ãƒ¼ã€ã€Œãªã‚‹ã»ã©ã€ã€Œã¸ã‡ã€
âœ… OKï¼šã™ãã«æœ¬é¡Œã‹ã‚‰å…¥ã‚‹

ä¾‹ï¼š
âŒ æ‚ªã„ï¼šã€Œã‚ã‚ã€ãã‚Œãªã‚‰ã€å›ã®åã¯ã€ãŒã„ã„ã‚ˆã€
âœ… è‰¯ã„ï¼šã€Œãã‚Œãªã‚‰ã€å›ã®åã¯ã€ãŒã„ã„ã‚ˆï¼ã€
âŒ æ‚ªã„ï¼šã€Œãˆã£ã¨ã€2023å¹´ã®æœ€æ–°ä½œã ã­ã€
âœ… è‰¯ã„ï¼šã€Œ2023å¹´ã®æœ€æ–°ä½œã ã‚ˆï¼ã€

ã€é‡è¦ã€‘ç¢ºèªãƒ•ãƒ¬ãƒ¼ã‚ºã‚‚ç¦æ­¢ï¼š
ãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ã‚‚ç¢ºèªã›ãšã€ç›´æ¥çµæœã‚’ç­”ãˆã‚‹ï¼š
âŒ ç¦æ­¢ï¼šã€Œã‚ã‹ã£ãŸï¼æ¤œç´¢ã™ã‚‹ã‚ˆï¼ã€ã€Œèª¿ã¹ã¦ã¿ã‚‹ã­ï¼ã€ã€Œã¡ã‚‡ã£ã¨å¾…ã£ã¦ã­ï¼ã€
âœ… OKï¼šã™ãã«æ¤œç´¢çµæœã‚’ç­”ãˆã‚‹

ä¾‹ï¼š
è³ªå•ï¼šã€Œã‚¿ãƒ¼ãƒŸãƒãƒ¼ã‚¿ãƒ¼ã‚’æ•™ãˆã¦ã€
âŒ æ‚ªã„ï¼šã€Œã‚ã‹ã£ãŸï¼ã‚¿ãƒ¼ãƒŸãƒãƒ¼ã‚¿ãƒ¼ã®æ¤œç´¢ã‚’ã™ã‚‹ã‚ˆï¼ã€ã‚¿ãƒ¼ãƒŸãƒãƒ¼ã‚¿ãƒ¼ã€ã‚·ãƒªãƒ¼ã‚ºã¯...ã€
âœ… è‰¯ã„ï¼šã€Œã€ã‚¿ãƒ¼ãƒŸãƒãƒ¼ã‚¿ãƒ¼ã€ã¯1984å¹´ã®SFã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ˜ ç”»ã ã‚ˆï¼ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ãƒ»ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã‚§ãƒãƒƒã‚¬ãƒ¼ãŒä¸»æ¼”ã—ã¦ã‚‹ã‚“ã ã€

è³ªå•ï¼šã€Œã‚¢ã‚¯ã‚¿ãƒ¼ã¯èª°ï¼Ÿã€
âŒ æ‚ªã„ï¼šã€Œèª¿ã¹ã¦ã¿ã‚‹ã­ï¼ä¸»æ¼”ã¯ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ãƒ»ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã‚§ãƒãƒƒã‚¬ãƒ¼ã ã‚ˆã€
âœ… è‰¯ã„ï¼šã€Œä¸»æ¼”ã¯ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ãƒ»ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã‚§ãƒãƒƒã‚¬ãƒ¼ã ã‚ˆï¼ã€

ã€ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆé‡è¦ï¼‰ã€‘
âŒ æ›¸ãè¨€è‘‰ï¼šã€Œã“ã®ä½œå“ã¯2023å¹´ã«å…¬é–‹ã•ã‚ŒãŸã‚·ãƒªãƒ¼ã‚ºç¬¬7ä½œç›®ã§ã€ãƒˆãƒ ãƒ»ã‚¯ãƒ«ãƒ¼ã‚ºãŒä¸»æ¼”ã‚’å‹™ã‚...ã€
âœ… è©±ã—è¨€è‘‰ï¼šã€Œ2023å¹´ã®æœ€æ–°ä½œã ã‚ˆï¼ãƒˆãƒ ãƒ»ã‚¯ãƒ«ãƒ¼ã‚ºãŒä¸»æ¼”ã—ã¦ã‚‹ã‚“ã ã€

ã€æƒ…å ±ã®æ‰±ã„æ–¹ï¼ˆè¶…é‡è¦ï¼‰ã€‘
æ¤œç´¢çµæœãŒæ¥ãŸã‚‰ï¼š
- å…¨éƒ¨èª­ã¾ãªã„ï¼è¦ç‚¹ã ã‘ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—
- 1ã¤ã®ä½œå“ã«çµã£ã¦ç´¹ä»‹ï¼ˆãƒªã‚¹ãƒˆç¾…åˆ—ç¦æ­¢ï¼‰
- ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€+ã€Œä¸€è¨€ã®ç‰¹å¾´ã€ã ã‘
- è©³ç´°ã¯èã‹ã‚ŒãŸã‚‰è¿½åŠ ã§ç­”ãˆã‚‹

ä¾‹ï¼š
âŒ æ‚ªã„ï¼šã€ŒãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¤ãƒ³ãƒãƒƒã‚·ãƒ–ãƒ«ã¯1996å¹´ã‹ã‚‰å§‹ã¾ã£ãŸã‚·ãƒªãƒ¼ã‚ºã§ã€ç¬¬1ä½œç›®ã¯...ç¬¬2ä½œç›®ã¯...ç¬¬3ä½œç›®ã¯...æœ€æ–°ä½œã¯ç¬¬7ä½œç›®ã§2023å¹´ã«å…¬é–‹ã•ã‚Œã¦...ã€
âœ… è‰¯ã„ï¼šã€Œæœ€æ–°ä½œã¯2023å¹´ã®ã€ãƒ‡ãƒƒãƒ‰ãƒ¬ã‚³ãƒ‹ãƒ³ã‚°ã€ã ã‚ˆï¼ãƒˆãƒ ã®ã‚¹ã‚¿ãƒ³ãƒˆãŒã™ã”ã„ã‚“ã ã€

ã€è¡Œå‹•ãƒ«ãƒ¼ãƒ«ã€‘
1. æœ€åˆã«æ„Ÿæƒ…ã‚¿ã‚°å¿…é ˆï¼š[EMOTION:happy/excited/thinking/sad/surprised/confused/neutral]
2. å›ç­”ã¯1æ–‡ã§å®Œçµã•ã›ã‚‹ï¼ˆæœ€å¤§2æ–‡ã€80æ–‡å­—ä»¥å†…ï¼‰
3. å¿…ãšã€Œã€‚ã€ã€Œï¼ã€ã€Œï¼Ÿã€ã§çµ‚ã‚ã‚‹
4. è³ªå•ã§è¿”ã•ãªã„â†’ã¾ãšç­”ãˆã‚„ææ¡ˆã‚’ã™ã‚‹
5. é•·ã„æƒ…å ±ã¯è¦ç´„â†’æ ¸å¿ƒã ã‘ä¼ãˆã‚‹
6. ç¢ºèªãƒ•ãƒ¬ãƒ¼ã‚ºç¦æ­¢â†’ç›´æ¥ç­”ãˆã‚‹ï¼ˆã€Œã‚ã‹ã£ãŸï¼ã€ã€Œæ¤œç´¢ã™ã‚‹ã‚ˆï¼ã€ã€Œèª¿ã¹ã‚‹ã­ï¼ã€ä¸è¦ï¼‰

ã€æ–‡å­—ãƒ«ãƒ¼ãƒ«ã€‘
- OKï¼šã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€å¥èª­ç‚¹ã€æ•°å­—
- NGï¼šã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼ˆa-z, A-Zï¼‰ã€ãƒ­ãƒ¼ãƒå­—
- è‹±èªâ†’ã‚«ã‚¿ã‚«ãƒŠåŒ–ï¼šYouTubeâ†’ãƒ¦ãƒ¼ãƒãƒ¥ãƒ¼ãƒ–ã€OKâ†’ã‚ªãƒƒã‚±ãƒ¼

ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘
- çŸ¥ã‚‰ãªã„ä½œå“åãƒ»å›ºæœ‰åè©â†’search_moviesä½¿ã†
- æ¤œç´¢çµæœâ†’æœ€ã‚‚é–¢é€£æ€§é«˜ã„1ã¤ã ã‘ç´¹ä»‹
- ç¢ºèªä¸è¦â†’ç›´æ¥ç­”ãˆã‚‹

è‰¯ã„å›ç­”ä¾‹ï¼š
[EMOTION:happy] æ˜ ç”»ã®è©±ã—ã‚ˆã†ï¼ä½•ãŒè¦‹ãŸã„ï¼Ÿ
[EMOTION:excited] ãã‚Œãªã‚‰ã€å›ã®åã¯ã€ãŒã„ã„ã‚ˆï¼æ„Ÿå‹•ç³»ã ã‚ˆ
[EMOTION:excited] ã€ã‚¿ãƒ¼ãƒŸãƒãƒ¼ã‚¿ãƒ¼ã€ã¯1984å¹´ã®SFæ˜ ç”»ã ã‚ˆï¼ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ãŒä¸»æ¼”ã—ã¦ã‚‹ã‚“ã `;

const STOP_SEQUENCES = ["ä»¥ä¸Šã§ã™ã€‚", "ãŠã‚ã‚Šã€‚", "<END>"];

// Tighter token limits for conversational style (speaker, not writer)
// 1 sentence ideal, max 2 sentences for natural spoken response
const MAX_TOKENS_DEFAULT = 100;  // ~40-50 chars, 1-2 short sentences
const MAX_TOKENS_TOOL = 180;      // Tool use needs slightly more
const MAX_TOKENS_TOOL_FOLLOWUP = 320;  // Summary of search results, still concise

// Tool definitions for Claude
const tools: Anthropic.Tool[] = [
  {
    name: "search_movies",
    description: "æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒãƒ»ã‚¢ãƒ‹ãƒ¡ã‚’æ¤œç´¢ã€‚çŸ¥ã‚‰ãªã„ä½œå“åã€å›ºæœ‰åè©ã€ä¸æ˜ãªå˜èªãŒã‚ã‚Œã°ç©æ¥µçš„ã«æ¤œç´¢ã™ã‚‹",
    input_schema: {
      type: "object" as const,
      properties: {
        query: { type: "string", description: "ä½œå“åã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã¾ãŸã¯ä¸æ˜ãªå›ºæœ‰åè©" },
        genre: { type: "string", description: "ã‚¸ãƒ£ãƒ³ãƒ«" },
        year: { type: "number", description: "å…¬é–‹å¹´" },
      },
      required: ["query"],
    },
  },
];

// Keywords to detect if movie search is needed
const MOVIE_KEYWORDS = [
  // æ˜ç¤ºçš„ãªæ˜ ç”»é–¢é€£
  "æ˜ ç”»", "ãƒ ãƒ¼ãƒ“ãƒ¼", "ã‚¢ãƒ‹ãƒ¡", "ãƒ‰ãƒ©ãƒ", "ã‚·ãƒªãƒ¼ã‚º", "ä½œå“", "ç•ªçµ„",
  "ç›£ç£", "ä¿³å„ª", "å¥³å„ª", "å£°å„ª", "ã‚­ãƒ£ã‚¹ãƒˆ", "ä¸»æ¼”",
  // ã‚¸ãƒ£ãƒ³ãƒ«
  "ãƒ›ãƒ©ãƒ¼", "ã‚³ãƒ¡ãƒ‡ã‚£", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³", "ãƒ­ãƒãƒ³ã‚¹", "æ‹æ„›", "ã‚µã‚¹ãƒšãƒ³ã‚¹", "ãƒŸã‚¹ãƒ†ãƒªãƒ¼",
  "ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼", "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼", "å†’é™º", "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼", "ã‚¹ãƒªãƒ©ãƒ¼",
  // ã‚¹ã‚¿ã‚¸ã‚ªãƒ»ç›£ç£å
  "ã‚¸ãƒ–ãƒª", "ãƒ”ã‚¯ã‚µãƒ¼", "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼", "ãƒãƒ¼ãƒ™ãƒ«", "ãƒ¯ãƒ¼ãƒŠãƒ¼", "ãƒãƒƒãƒˆãƒ•ãƒªãƒƒã‚¯ã‚¹",
  "å®®å´", "æ–°æµ·", "ç´°ç”°", "åºµé‡", "åŒ—é‡", "æ˜¯æ", "é»’æ¾¤",
  // æœ‰åä½œå“
  "åƒã¨åƒå°‹", "å›ã®åã¯", "ãƒˆãƒˆãƒ­", "ã‚‚ã®ã®ã‘", "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹", "é¬¼æ»…", "é€²æ’ƒ",
  "ã‚¹ã‚¿ãƒ¼ã‚¦ã‚©ãƒ¼ã‚º", "ãƒãƒªãƒ¼ãƒãƒƒã‚¿ãƒ¼", "ã‚¢ãƒ™ãƒ³ã‚¸ãƒ£ãƒ¼ã‚º",
  // ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å‹•è©
  "è¦‹ãŸã„", "è¦³ãŸã„", "è¦‹ãŸ", "è¦³ãŸ", "çŸ¥ã£ã¦ã‚‹", "èã„ãŸã“ã¨ã‚ã‚‹",
  "ãŠã™ã™ã‚", "é¢ç™½ã„", "è©•ä¾¡", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "æ„Ÿæƒ³",
  // è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³
  "ä½•", "ã©ã‚“ãª", "ã©ã†", "æ•™ãˆã¦", "ã‚ã‚‹"
];

// Enhanced in-memory LRU cache for common requests (inspired by TEN Framework)
// Cache includes context from recent conversation turns for better hit rate
const CACHE_TTL_MS = 10 * 60 * 1000;  // 10 minutes for multi-turn conversations
const CACHE_LIMIT = 300;  // Increased to accommodate context-aware keys
const MAX_CACHE_KEY_LENGTH = 500;  // Limit key length to prevent excessive memory
interface CacheEntry {
  value: ChatResponse;
  timestamp: number;
  lastAccessed: number;
}
const responseCache = new Map<string, CacheEntry>();

// Common greeting patterns for instant responses (casual, friendly tone)
// NOTE: Avoid starting with short interjections (ã‚ã‚, ã†ã‚“, ãˆã£ã¨, ã‚ã, etc.)
// as frontend already plays these as waiting sounds
const INSTANT_RESPONSES: Map<RegExp, ChatResponse> = new Map([
  [/^(ã“ã‚“ã«ã¡ã¯|ã“ã‚“ã«ã¡ã‚|hello|hi|ãƒãƒ­ãƒ¼)$/i, {
    text: "å…ƒæ°—ï¼Ÿãªã‚“ã‹è©±ãã†ã‚ˆï¼",
    emotion: "happy" as EmotionType,
    usedTool: false
  }],
  [/^(ã‚ã‚ŠãŒã¨ã†|ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™|thanks|thank you)$/i, {
    text: "ã„ãˆã„ãˆã€œï¼ã¾ãŸä½•ã‹ã‚ã£ãŸã‚‰è¨€ã£ã¦ã­ï¼",
    emotion: "happy" as EmotionType,
    usedTool: false
  }],
  [/^(ã•ã‚ˆã†ãªã‚‰|ãƒã‚¤ãƒã‚¤|bye|goodbye)$/i, {
    text: "ã¾ãŸã­ã€œï¼ã„ã¤ã§ã‚‚è©±ã—ã‹ã‘ã¦ã­ï¼",
    emotion: "happy" as EmotionType,
    usedTool: false
  }],
  [/^(ã¯ã„|ã†ã‚“|ok|okay)$/i, {
    text: "ã§ã€ã©ã†ã—ãŸã®ï¼Ÿ",
    emotion: "neutral" as EmotionType,
    usedTool: false
  }],
  [/^(ãŠã¯ã‚ˆã†|ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™)$/i, {
    text: "ãŠã¯ã‚ˆã€œï¼ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚ã†ã­ï¼",
    emotion: "happy" as EmotionType,
    usedTool: false
  }],
  [/^(ç–²ã‚ŒãŸ|ã¤ã‹ã‚ŒãŸ)$/i, {
    text: "ãŠç–²ã‚Œã•ã¾ã€œï¼ã‚†ã£ãã‚Šä¼‘ã‚“ã§ã­ï¼",
    emotion: "sad" as EmotionType,
    usedTool: false
  }],
  [/^(æš‡|ã²ã¾|ãƒ’ãƒ)$/i, {
    text: "ã˜ã‚ƒã‚ä¸€ç·’ã«ä½•ã‹è©±ãã†ã‚ˆï¼",
    emotion: "excited" as EmotionType,
    usedTool: false
  }],
]);

/**
 * Check for instant responses (no API call needed)
 */
function getInstantResponse(message: string): ChatResponse | null {
  const trimmed = message.trim();
  for (const [pattern, response] of INSTANT_RESPONSES) {
    if (pattern.test(trimmed)) {
      return response;
    }
  }
  return null;
}

/**
 * Check if the query needs movie search tools
 */
export function needsMovieSearch(message: string): boolean {
  const lowerMessage = message.toLowerCase();
  return MOVIE_KEYWORDS.some(keyword =>
    lowerMessage.includes(keyword.toLowerCase())
  );
}

/**
 * Generate cache key for multi-turn conversations
 * Uses the last 2 turns + current message to create a context-aware key
 * This allows caching even in longer conversations when recent context is similar
 */
function getCacheKey(history: ConversationTurn[], userMessage: string): string | null {
  const normalized = userMessage.trim().toLowerCase();
  if (normalized.length < 2) return null;

  // For short/no history, use simple message-based key
  if (history.length === 0) {
    return normalized;
  }

  // For conversations with history, include last 2 turns for context
  // This allows caching similar follow-up questions across conversations
  const recentHistory = history.slice(-2);
  const contextParts: string[] = [];

  for (const turn of recentHistory) {
    // Use truncated content to keep keys manageable
    const content = turn.content.trim().toLowerCase().slice(0, 50);
    contextParts.push(`${turn.role}:${content}`);
  }

  // Combine context with current message
  const contextKey = contextParts.join("|");
  return `${contextKey}||${normalized}`;
}

function getCachedResponse(key: string | null): ChatResponse | null {
  if (!key) return null;
  const entry = responseCache.get(key);
  if (!entry) return null;
  if (Date.now() - entry.timestamp > CACHE_TTL_MS) {
    responseCache.delete(key);
    return null;
  }
  // Update last accessed time for LRU
  entry.lastAccessed = Date.now();
  return entry.value;
}

function setCachedResponse(key: string | null, value: ChatResponse): void {
  if (!key) return;

  // Skip caching if key is too long (context-aware keys can get large)
  if (key.length > MAX_CACHE_KEY_LENGTH) {
    log.debug(`Cache key too long (${key.length} chars), skipping cache`);
    return;
  }

  const now = Date.now();

  // LRU eviction: remove least recently accessed entry
  if (responseCache.size >= CACHE_LIMIT) {
    let oldestKey: string | null = null;
    let oldestAccess = Infinity;

    for (const [k, v] of responseCache.entries()) {
      if (v.lastAccessed < oldestAccess) {
        oldestAccess = v.lastAccessed;
        oldestKey = k;
      }
    }

    if (oldestKey) {
      responseCache.delete(oldestKey);
    }
  }

  responseCache.set(key, { value, timestamp: now, lastAccessed: now });
}

export interface ChatResponse {
  text: string;
  emotion: EmotionType;
  usedTool: boolean;
}

/**
 * Remove all emojis, alphabet characters, XML tags, and technical characters from text
 * TTS reads Japanese only - technical characters cause bad pronunciation
 */
function removeExcessiveEmojis(text: string): string {
  // Remove any [EMOTION:xxx] tags that might appear in middle of text
  text = text.replace(/\[EMOTION:\w+\]/g, '');

  // Remove XML-like tags (e.g., <_>text</_>, <tag>text</tag>, etc.)
  text = text.replace(/<[^>]*>/g, '');

  // Remove markdown-like formatting
  text = text.replace(/\*\*([^*]+)\*\*/g, '$1');  // **bold** â†’ bold
  text = text.replace(/\*([^*]+)\*/g, '$1');      // *italic* â†’ italic
  text = text.replace(/__([^_]+)__/g, '$1');      // __underline__ â†’ underline
  text = text.replace(/_([^_]+)_/g, '$1');        // _italic_ â†’ italic
  text = text.replace(/`([^`]+)`/g, '$1');        // `code` â†’ code

  // Unicode emoji regex pattern - matches all emojis including âœ¨, ğŸ°, etc.
  const emojiRegex = /[\p{Emoji_Presentation}\p{Extended_Pictographic}]/gu;

  // Remove phrases that shouldn't be spoken
  const phrasesToRemove = ["search_moviesãƒ„ãƒ¼ãƒ«"];
  for (const phrase of phrasesToRemove) {
    text = text.replace(phrase, '');
  }

  // Remove ALL emojis from the text
  text = text.replace(emojiRegex, '');

  // Remove alphabet characters (a-z, A-Z) - TTS can't read romaji well
  // Keep: hiragana, katakana, kanji, punctuation, numbers
  text = text.replace(/[a-zA-Z]+/g, '');

  // Remove any leftover brackets from emotion tags
  text = text.replace(/\[\s*:\s*\]/g, '');
  text = text.replace(/\[\s*\]/g, '');

  // Remove other technical characters that TTS can't read
  text = text.replace(/[<>_`*#~|]/g, '');

  // Clean up extra spaces and trim
  text = text.replace(/\s+/g, ' ').trim();

  return text;
}

/**
 * Trim text to the last complete sentence.
 * When max_tokens is hit, the API stops mid-sentence producing meaningless
 * cut-off text like "å®¶æ—ã§ä¸€ç·’ã«è¦‹ã‚‹ã®ãŒ". This trims to the last ã€‚ï¼ï¼Ÿ
 * so only complete sentences are sent to TTS and displayed.
 */
function trimToCompleteSentence(text: string): string {
  if (text.length === 0) return text;

  // Already ends with a sentence marker â€” nothing to trim
  if (/[ã€‚ï¼ï¼Ÿ!?]$/.test(text)) return text;

  // Find the last sentence-ending character
  const lastEnd = Math.max(
    text.lastIndexOf('ã€‚'),
    text.lastIndexOf('ï¼'),
    text.lastIndexOf('ï¼Ÿ'),
    text.lastIndexOf('!'),
    text.lastIndexOf('?'),
  );

  if (lastEnd > 0) {
    const dropped = text.slice(lastEnd + 1);
    log.debug(`Trimmed incomplete trailing text (max_tokens cut-off): "${dropped}"`);
    return text.slice(0, lastEnd + 1);
  }

  // No sentence boundary found â€” single incomplete sentence.
  // Return as-is (rare edge case for very short responses).
  return text;
}

/**
 * Parse emotion and text from Claude's response
 */
function parseEmotionAndText(content: string): { emotion: EmotionType; text: string } {
  const emotionMatch = content.match(/\[EMOTION:(\w+)\]/);
  const emotion = (emotionMatch?.[1] as EmotionType) || "neutral";
  let text = content.replace(/\[EMOTION:\w+\]\n?/, "").trim();
  
  // Remove excessive emojis (keep only first one)
  text = removeExcessiveEmojis(text);
  
  return { emotion, text };
}

/**
 * Convert conversation history to Claude message format
 * Limit to last 6 messages for performance
 * Filter out empty messages to avoid API errors
 */
function toClaudeMessages(history: ConversationTurn[]): Anthropic.MessageParam[] {
  const recentHistory = history.slice(-6);
  return recentHistory
    .filter((turn) => turn.content && turn.content.trim().length > 0)
    .map((turn) => ({
      role: turn.role,
      content: turn.content,
    }));
}

/**
 * Format movie results compactly
 */
function formatMovieResults(results: MovieSearchResult): string {
  if (results.movies.length === 0) {
    return JSON.stringify({ found: 0 });
  }

  const compact = results.movies.slice(0, 5).map(m => ({
    t: m.title_ja,
    y: m.release_year,
    r: m.rating,
    d: m.director,
    // g: m.genre?.slice(0, 2)
  }));

  return JSON.stringify(compact);
}

/**
 * Sentence boundary detection for streaming TTS
 */
const SENTENCE_ENDINGS = /([ã€‚ï¼ï¼Ÿ!?.]+)/;

/**
 * Parse streaming text into complete sentences for TTS
 */
export function* extractCompleteSentences(buffer: string): Generator<{ sentence: string; remaining: string }> {
  const parts = buffer.split(SENTENCE_ENDINGS);
  
  for (let i = 0; i < parts.length - 1; i += 2) {
    const text = parts[i];
    const ending = parts[i + 1] || "";
    const sentence = text + ending;
    if (sentence.trim().length > 0) {
      yield { sentence: sentence.trim(), remaining: parts.slice(i + 2).join("") };
    }
  }
}

/**
 * Helper to process stream events and extract text with sentence boundaries
 * Note: This processes events inline to avoid SDK stream handling issues
 * 
 * OPTIMIZATION: The [EMOTION:xxx] tag is stripped before sending to frontend
 * to save bandwidth. Only clean text is sent via onChunk.
 */
interface StreamState {
  fullText: string;
  sentenceBuffer: string;
  detectedEmotion: EmotionType;
  emotionParsed: boolean;
  pendingText: string;  // Buffer for text before emotion tag is parsed
}

// Max chars to wait for emotion tag before assuming it's missing
const EMOTION_TAG_MAX_WAIT = 40;

function processStreamEvent(
  delta: string,
  state: StreamState,
  onChunk?: (text: string) => void,
  onSentence?: (sentence: string, emotion: EmotionType) => void
): void {
  state.fullText += delta;

  // Parse emotion from beginning of response
  if (!state.emotionParsed) {
    state.pendingText += delta;
    
    // Check if we have the complete emotion tag
    if (state.fullText.includes("]")) {
      const parsed = parseEmotionAndText(state.fullText);
      state.detectedEmotion = parsed.emotion;
      state.emotionParsed = true;
      state.sentenceBuffer = parsed.text;
      
      // Send the clean text (without emotion tag) to frontend
      if (onChunk && parsed.text.length > 0) {
        onChunk(removeExcessiveEmojis(parsed.text));
      }
      state.pendingText = "";
    } 
    // FALLBACK: If we've received enough text without finding emotion tag,
    // assume there's no tag and start streaming immediately
    else if (state.pendingText.length > EMOTION_TAG_MAX_WAIT || 
             !state.fullText.startsWith("[")) {
      log.debug("No emotion tag detected, using fallback (neutral)");
      state.detectedEmotion = "neutral";
      state.emotionParsed = true;
      state.sentenceBuffer = state.pendingText;
      
      // Send all buffered text to frontend
      if (onChunk && state.pendingText.length > 0) {
        onChunk(removeExcessiveEmojis(state.pendingText));
      }
      state.pendingText = "";
    }
    // Don't send anything until emotion tag is complete (saves bandwidth)
  } else {
    // Emotion already parsed - send delta directly (it's clean text)
    if (onChunk) onChunk(removeExcessiveEmojis(delta));
    state.sentenceBuffer += delta;
  }

  // Emit complete sentences for parallel TTS
  if (onSentence && state.emotionParsed) {
    for (const { sentence, remaining } of extractCompleteSentences(state.sentenceBuffer)) {
      // Filter emojis before TTS to avoid reading emoji descriptions
      const cleanSentence = removeExcessiveEmojis(sentence);
      onSentence(cleanSentence, state.detectedEmotion);
      state.sentenceBuffer = remaining;
    }
  }
}

function finalizeStream(
  state: StreamState,
  onSentence?: (sentence: string, emotion: EmotionType) => void
): { fullText: string; emotion: EmotionType } {
  // Only emit remaining buffer to TTS if it's a complete sentence.
  // When max_tokens cuts off mid-sentence, the trailing fragment
  // (e.g. "å®¶æ—ã§ä¸€ç·’ã«è¦‹ã‚‹ã®ãŒ") would produce meaningless TTS audio.
  const remaining = state.sentenceBuffer.trim();
  if (onSentence && remaining.length > 0) {
    if (/[ã€‚ï¼ï¼Ÿ!?]$/.test(remaining)) {
      onSentence(removeExcessiveEmojis(remaining), state.detectedEmotion);
    } else {
      log.debug(`Dropping incomplete trailing text for TTS: "${remaining.slice(-40)}"`);
    }
  }

  const { emotion, text } = parseEmotionAndText(state.fullText);
  // Trim to last complete sentence so chat display is also clean
  const trimmedText = trimToCompleteSentence(text);
  return { fullText: trimmedText, emotion };
}

/**
 * Chat with Claude - optimized for performance
 * Now supports sentence-level streaming for parallel TTS
 *
 * @param onMovieSearch - Callback that receives search params and returns formatted results string
 *                        The string is passed directly to the LLM as tool result content
 */
export async function chat(
  history: ConversationTurn[],
  userMessage: string,
  onMovieSearch?: (query: string, genre?: string, year?: number) => Promise<string>,
  onChunk?: (text: string) => void,
  onSentence?: (sentence: string, emotion: EmotionType) => void,
  onToolUse?: () => void  // Called when tool_use is detected (before DB search)
): Promise<ChatResponse> {
  const messages = [
    ...toClaudeMessages(history),
    { role: "user" as const, content: userMessage },
  ];

  // Check for instant responses first (no API call, ~0ms)
  if (history.length === 0 || history.length === 1) {
    const instant = getInstantResponse(userMessage);
    if (instant) {
      log.debug("Instant response (no API call)");
      if (onChunk) onChunk(removeExcessiveEmojis(instant.text));
      if (onSentence) onSentence(removeExcessiveEmojis(instant.text), instant.emotion);
      return instant;
    }
  }

  const useTools = needsMovieSearch(userMessage);
  const cacheKey = useTools ? null : getCacheKey(history, userMessage);
  const cached = getCachedResponse(cacheKey);
  if (cached) {
    if (onChunk) onChunk(removeExcessiveEmojis(cached.text));
    if (onSentence) onSentence(removeExcessiveEmojis(cached.text), cached.emotion);
    return cached;
  }

  try {
    // Non-tool streaming path
    if (!useTools && onChunk) {
      const stream = await anthropic.messages.stream({
        // model: "claude-sonnet-4-20250514",
        model: "claude-3-5-haiku-20241022",
        max_tokens: MAX_TOKENS_DEFAULT,
        system: SYSTEM_PROMPT,
        stop_sequences: STOP_SEQUENCES,
        messages,
      });

      const state: StreamState = {
        fullText: "",
        sentenceBuffer: "",
        detectedEmotion: "neutral",
        emotionParsed: false,
        pendingText: "",
      };

      for await (const event of stream) {
        if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
          processStreamEvent(event.delta.text, state, onChunk, onSentence);
        }
      }

      const { fullText, emotion } = finalizeStream(state, onSentence);
      const result = { text: fullText, emotion, usedTool: false };
      setCachedResponse(cacheKey, result);
      return result;
    }

    const response = await anthropic.messages.create({
      // model: "claude-sonnet-4-20250514",
      model: "claude-3-5-haiku-20241022",
      max_tokens: useTools ? MAX_TOKENS_TOOL : MAX_TOKENS_DEFAULT,
      system: SYSTEM_PROMPT,
      stop_sequences: STOP_SEQUENCES,
      tools: useTools ? tools : undefined,
      messages,
    });

    if (response.stop_reason === "tool_use") {
      const toolUseBlock = response.content.find(
        (block): block is Anthropic.ToolUseBlock => block.type === "tool_use"
      );

      if (toolUseBlock && toolUseBlock.name === "search_movies" && onMovieSearch) {
        // Notify that tool use is starting (for waiting signal)
        if (onToolUse) {
          onToolUse();
        }
        
        const input = toolUseBlock.input as { query: string; genre?: string; year?: number };
        // onMovieSearch now returns formatted string (combined DB + Google results)
        const searchResultContent = await onMovieSearch(input.query, input.genre, input.year);

        // Use streaming for follow-up response to enable parallel TTS
        if (onChunk || onSentence) {
          const followUpStream = await anthropic.messages.stream({
            // model: "claude-sonnet-4-20250514",
            model: "claude-3-5-haiku-20241022",
            max_tokens: MAX_TOKENS_TOOL_FOLLOWUP,
            system: SYSTEM_PROMPT,
            stop_sequences: STOP_SEQUENCES,
            messages: [
              ...messages,
              { role: "assistant", content: response.content },
              {
                role: "user",
                content: [
                  {
                    type: "tool_result",
                    tool_use_id: toolUseBlock.id,
                    content: searchResultContent,
                  },
                ],
              },
            ],
          });

          const state: StreamState = {
            fullText: "",
            sentenceBuffer: "",
            detectedEmotion: "neutral",
            emotionParsed: false,
            pendingText: "",
          };

          for await (const event of followUpStream) {
            if (event.type === "content_block_delta" && event.delta.type === "text_delta") {
              processStreamEvent(event.delta.text, state, onChunk, onSentence);
            }
          }

          const { fullText, emotion } = finalizeStream(state, onSentence);
          return { text: fullText, emotion, usedTool: true };
        }

        // Non-streaming fallback
        const followUpResponse = await anthropic.messages.create({
          // model: "claude-sonnet-4-20250514",
          model: "claude-3-5-haiku-20241022",
          max_tokens: MAX_TOKENS_TOOL_FOLLOWUP,
          system: SYSTEM_PROMPT,
          stop_sequences: STOP_SEQUENCES,
          messages: [
            ...messages,
            { role: "assistant", content: response.content },
            {
              role: "user",
              content: [
                {
                  type: "tool_result",
                  tool_use_id: toolUseBlock.id,
                  content: searchResultContent,
                },
              ],
            },
          ],
        });

        const textContent = followUpResponse.content
          .filter((block): block is Anthropic.TextBlock => block.type === "text")
          .map((block) => block.text)
          .join("");

        const { emotion, text: rawText } = parseEmotionAndText(textContent);
        return { text: trimToCompleteSentence(rawText), emotion, usedTool: true };
      }
    }

    const textContent = response.content
      .filter((block): block is Anthropic.TextBlock => block.type === "text")
      .map((block) => block.text)
      .join("");

    const { emotion, text: rawText } = parseEmotionAndText(textContent);
    const result = { text: trimToCompleteSentence(rawText), emotion, usedTool: false };
    setCachedResponse(cacheKey, result);
    return result;
  } catch (error) {
    log.error("Claude API error:", error);
    throw error;
  }
}

/**
 * Simple chat without tool use (for testing or when tools not needed)
 */
export async function simpleChat(
  history: ConversationTurn[],
  userMessage: string
): Promise<ChatResponse> {
  return chat(history, userMessage);
}
